<div class="entry-wrapper">
		<header class="entry-header">
			<h1 class="entry-title">Dropout and early stopping with&nbsp;Adagrad</h1>
							<div class="entry-meta">
					<span class="posted-on">On <a href="https://mhayml.wordpress.com/2017/07/19/dropout-and-early-stopping-with-adagrad/" rel="bookmark"><time class="entry-date published" datetime="2017-07-19T22:03:24+00:00">July 19, 2017</time><time class="updated" datetime="2020-10-01T21:14:14+00:00">October 1, 2020</time></a></span><span class="byline"> By <span class="author vcard"><a class="url fn n" href="https://mhayml.wordpress.com/author/mhayml/">Mike Hay</a></span></span><span class="edit-link"><a class="post-edit-link" href="https://wordpress.com/post/mhayml.wordpress.com/450">Edit</a></span>				</div><!-- .entry-meta -->
					</header><!-- .entry-header -->

		<div class="entry-content">
			<p>I was experimenting with applying variational dropout, using multiplicative Gaussian output noising, to primal generalized Gaussian process models. I&#8217;m going to show an example of Gaussian process classification with dropout, and show that it gives nearly the same performance as early stopping with Adagrad. This is because of some surprising connections between Adagrad and dropout</p>
<p>In these models, we are trying to minimize the following quantity,</p>
<p><img src="https://s0.wp.com/latex.php?latex=%5Cmathbb%7BE%7D_q%5Cleft%5B%5Csum_i+L%28%5Cmathbf%7B%5Chat%7By_i%7D%7D%2C%5Cmathbf%7By_i%7D%29%5Cright%5D&#038;bg=ffffff&#038;fg=111111&#038;s=0&#038;c=20201002" alt="&#92;mathbb{E}_q&#92;left[&#92;sum_i L(&#92;mathbf{&#92;hat{y_i}},&#92;mathbf{y_i})&#92;right]" title="&#92;mathbb{E}_q&#92;left[&#92;sum_i L(&#92;mathbf{&#92;hat{y_i}},&#92;mathbf{y_i})&#92;right]" class="latex" /> (1),</p>
<p>where <img src="https://s0.wp.com/latex.php?latex=%5Cmathbf%7B%5Chat%7By%7D%7D+%3D%281+%2B+%5Cmathbf%7BA%7D%29+%5Cmathbf%7BK+%5Calpha%7D&#038;bg=ffffff&#038;fg=111111&#038;s=0&#038;c=20201002" alt="&#92;mathbf{&#92;hat{y}} =(1 + &#92;mathbf{A}) &#92;mathbf{K &#92;alpha}" title="&#92;mathbf{&#92;hat{y}} =(1 + &#92;mathbf{A}) &#92;mathbf{K &#92;alpha}" class="latex" /> is the vector of observations of the dependent variable we are trying to predict (or a matrix for multi-output regression or classification). This gives multiplicative noise, rather than additive noise as with standard L2 regularization. Here, <img src="https://s0.wp.com/latex.php?latex=%5Cmathbf%7BK%7D&#038;bg=ffffff&#038;fg=111111&#038;s=0&#038;c=20201002" alt="&#92;mathbf{K}" title="&#92;mathbf{K}" class="latex" /> is the kernel matrix, from some positive-definite kernel function defining a reproducing kernel Hilbert space (RKHS). <img src="https://s0.wp.com/latex.php?latex=%5Cmathbf%7B%5Calpha%7D&#038;bg=ffffff&#038;fg=111111&#038;s=0&#038;c=20201002" alt="&#92;mathbf{&#92;alpha}" title="&#92;mathbf{&#92;alpha}" class="latex" /> are function space weights. By the representer theorem, the function <img src="https://s0.wp.com/latex.php?latex=%5Chat%7Bf%7D&#038;bg=ffffff&#038;fg=111111&#038;s=0&#038;c=20201002" alt="&#92;hat{f}" title="&#92;hat{f}" class="latex" /> minimizing the above quantity can be written as a linear combination of the basis functions at each training point, where <img src="https://s0.wp.com/latex.php?latex=%5Cmathbf%7B%5Calpha%7D&#038;bg=ffffff&#038;fg=111111&#038;s=0&#038;c=20201002" alt="&#92;mathbf{&#92;alpha}" title="&#92;mathbf{&#92;alpha}" class="latex" /> are the weights. The matrix <img src="https://s0.wp.com/latex.php?latex=%5Cmathbf%7BA%7D&#038;bg=ffffff&#038;fg=111111&#038;s=0&#038;c=20201002" alt="&#92;mathbf{A}" title="&#92;mathbf{A}" class="latex" /> is diagonal whose entries are normally distributed with mean zero and some standard deviation <img src="https://s0.wp.com/latex.php?latex=%5Csigma+&#038;bg=ffffff&#038;fg=111111&#038;s=0&#038;c=20201002" alt="&#92;sigma " title="&#92;sigma " class="latex" />.</p>
<p><a href="https://arxiv.org/abs/1506.02557">Kingma et al. 2015</a> showed that dropout with multiplicative weight noising is equivalent to doing variational inference with a log-uniform Jefferys prior. They also explored variational dropout, where the noise is placed on layer outputs instead of weights themselves. Weight noise only affects the objective function through the output anyway. Kingma et al. introduced a form of dropout with multiplicative output noising that exactly matches multiplicative weight noising in expectation, by taking into account correlations of output variables. Unlike Kigma et al., I&#8217;m ignoring these correlations. For single-output models, it&#8217;s the same thing anyway, since there is only one output dimension anyway. The advantage of this is that we can think of it as noise on our dependent variable specifically. In the context of generalized Gaussian process regression, I this formulation is more natural.</p>
<p>As I mentioned, Jefferys priors are invariant to reparameterizations. This invariance explains why dropout works so well compared to L2 regularization for deep neural networks: You have no idea of the scale of the weights, since their effect on the objective function is only through one or more nonlinear transformations. An invariant prior makes no assumption of the <em>scale</em> of the weights, on the other hand. For linear regression, you can get the same invariance by just whitening your data ahead of time. However, this doesn&#8217;t work for GLMs. Hence, dropout could still be useful.</p>
<h2 id="Stochastic-variational-inference">Doubly stochastic variational inference</h2>
<p>My main motivation trying this model is to test the effectiveness of multiplicative output noising for stochastic variational Bayes. Gaussian process regression with a Gaussian prior is a conjugate model. We can exactly, and tractably, get the posterior distribution. This doesn&#8217;t hold for much else, though. Other error models, corresponding to log-loss, robust Student-t loss, etc., are usually not conjugate to a Gaussian prior, nevermind a Jefferys prior. With non-conjugate models, it is usually computationally infeasible to calculate the normalization constant because it involves a high-dimensional integral.</p>
<p>Stochastic variational inference handles this problem elegantly. To review, variational inference in general tries to approximate an intractable posterior distribution <img src="https://s0.wp.com/latex.php?latex=P%28z%7Cx%29&#038;bg=ffffff&#038;fg=111111&#038;s=0&#038;c=20201002" alt="P(z|x)" title="P(z|x)" class="latex" /> with a simpler distribution <img src="https://s0.wp.com/latex.php?latex=Q%28z%29&#038;bg=ffffff&#038;fg=111111&#038;s=0&#038;c=20201002" alt="Q(z)" title="Q(z)" class="latex" /> from some restricted family. The idea is to fit <img src="https://s0.wp.com/latex.php?latex=Q%28z%29&#038;bg=ffffff&#038;fg=111111&#038;s=0&#038;c=20201002" alt="Q(z)" title="Q(z)" class="latex" /> (based on whatever variational parameters it has) such that it is as close to <img src="https://s0.wp.com/latex.php?latex=P%28z%7Cx%29&#038;bg=ffffff&#038;fg=111111&#038;s=0&#038;c=20201002" alt="P(z|x)" title="P(z|x)" class="latex" /> as possible. &#8220;Closeness&#8221; is defined as KL divergence of <img src="https://s0.wp.com/latex.php?latex=P%28z%7Cx%29&#038;bg=ffffff&#038;fg=111111&#038;s=0&#038;c=20201002" alt="P(z|x)" title="P(z|x)" class="latex" /> <em>from</em> <img src="https://s0.wp.com/latex.php?latex=Q%28z%29&#038;bg=ffffff&#038;fg=111111&#038;s=0&#038;c=20201002" alt="Q(z)" title="Q(z)" class="latex" />:</p>
<p><img src="https://s0.wp.com/latex.php?latex=%5Cmathrm%7BD_%7BKL%7D%7D%28Q%7C%7CP%29+%3D+%5Cmathbb%7BE%7D_Q+%5Cleft%5B+%5Clog+P%28z%7Cx%29+-+%5Clog+Q%28z%29%29+%2B+%5Clog+P%28x%29+%5Cright%5D&#038;bg=ffffff&#038;fg=111111&#038;s=0&#038;c=20201002" alt="&#92;mathrm{D_{KL}}(Q||P) = &#92;mathbb{E}_Q &#92;left[ &#92;log P(z|x) - &#92;log Q(z)) + &#92;log P(x) &#92;right]" title="&#92;mathrm{D_{KL}}(Q||P) = &#92;mathbb{E}_Q &#92;left[ &#92;log P(z|x) - &#92;log Q(z)) + &#92;log P(x) &#92;right]" class="latex" /><br />
(2)</p>
<p>Note that <img src="https://s0.wp.com/latex.php?latex=%5Clog+P%28x%29&#038;bg=ffffff&#038;fg=111111&#038;s=0&#038;c=20201002" alt="&#92;log P(x)" title="&#92;log P(x)" class="latex" /> (known as the log evidence) is fixed with respect to <img src="https://s0.wp.com/latex.php?latex=z&#038;bg=ffffff&#038;fg=111111&#038;s=0&#038;c=20201002" alt="z" title="z" class="latex" />. Removing this term gives <em>negative</em> of the evidence lower bound, or ELBO. We will denote this by <img src="https://s0.wp.com/latex.php?latex=%5Cmathcal%7BL%7D%28Q%29&#038;bg=ffffff&#038;fg=111111&#038;s=0&#038;c=20201002" alt="&#92;mathcal{L}(Q)" title="&#92;mathcal{L}(Q)" class="latex" />. Maximizing the ELBO gives the optimal <img src="https://s0.wp.com/latex.php?latex=Q%28z%29&#038;bg=ffffff&#038;fg=111111&#038;s=0&#038;c=20201002" alt="Q(z)" title="Q(z)" class="latex" />. We can also decompose the posterior <img src="https://s0.wp.com/latex.php?latex=%5Clog+P%28z%7Cx%29&#038;bg=ffffff&#038;fg=111111&#038;s=0&#038;c=20201002" alt="&#92;log P(z|x)" title="&#92;log P(z|x)" class="latex" /> into a prior term <img src="https://s0.wp.com/latex.php?latex=F%28z%29&#038;bg=ffffff&#038;fg=111111&#038;s=0&#038;c=20201002" alt="F(z)" title="F(z)" class="latex" /> and a likelihood term  <img src="https://s0.wp.com/latex.php?latex=H%28z%7Cx%29&#038;bg=ffffff&#038;fg=111111&#038;s=0&#038;c=20201002" alt="H(z|x)" title="H(z|x)" class="latex" />. We can then rewrite the ELBO as follows:</p>
<p><img src="https://s0.wp.com/latex.php?latex=%5Cmathcal%7BL%7D%28Q%29+%3D+%5Cmathbb%7BE%7D_Q+%5Cleft%5B+%5Clog+H%28z%7Cx%29+%2B+%5Clog+F%28z%29+-+%5Clog+Q%28z%29+%5Cright%5D&#038;bg=ffffff&#038;fg=111111&#038;s=0&#038;c=20201002" alt="&#92;mathcal{L}(Q) = &#92;mathbb{E}_Q &#92;left[ &#92;log H(z|x) + &#92;log F(z) - &#92;log Q(z) &#92;right]" title="&#92;mathcal{L}(Q) = &#92;mathbb{E}_Q &#92;left[ &#92;log H(z|x) + &#92;log F(z) - &#92;log Q(z) &#92;right]" class="latex" /> (3),</p>
<p>or,</p>
<p><img src="https://s0.wp.com/latex.php?latex=%5Cmathcal%7BL%7D%28Q%29+%3D+%5Cmathbb%7BE%7D_Q+%5Cleft%5B+%5Clog+H%28z%7Cx%29%5Cright%5D+%2B+%5Cmathrm%7BD_%7BKL%7D%7D%28Q%7C%7CF%29&#038;bg=ffffff&#038;fg=111111&#038;s=0&#038;c=20201002" alt="&#92;mathcal{L}(Q) = &#92;mathbb{E}_Q &#92;left[ &#92;log H(z|x)&#92;right] + &#92;mathrm{D_{KL}}(Q||F)" title="&#92;mathcal{L}(Q) = &#92;mathbb{E}_Q &#92;left[ &#92;log H(z|x)&#92;right] + &#92;mathrm{D_{KL}}(Q||F)" class="latex" /> (4),</p>
<p>Here, we can see that maximizing the ELBO tries make the expectation of the log-likelihood <img src="https://s0.wp.com/latex.php?latex=%5Clog+H%28z%7Cx%29&#038;bg=ffffff&#038;fg=111111&#038;s=0&#038;c=20201002" alt="&#92;log H(z|x)" title="&#92;log H(z|x)" class="latex" /> big. But it also tries to keep <img src="https://s0.wp.com/latex.php?latex=Q%28z%29&#038;bg=ffffff&#038;fg=111111&#038;s=0&#038;c=20201002" alt="Q(z)" title="Q(z)" class="latex" /> close to the prior <img src="https://s0.wp.com/latex.php?latex=F%28z%29&#038;bg=ffffff&#038;fg=111111&#038;s=0&#038;c=20201002" alt="F(z)" title="F(z)" class="latex" />.</p>
<p>For our case of generalized Gaussian process models with output noising, we have a log-uniform prior <img src="https://s0.wp.com/latex.php?latex=F%28%5Clog%28%7Cy%7C%29%29+%5Cpropto+c&#038;bg=ffffff&#038;fg=111111&#038;s=0&#038;c=20201002" alt="F(&#92;log(|y|)) &#92;propto c" title="F(&#92;log(|y|)) &#92;propto c" class="latex" /> on the output <img src="https://s0.wp.com/latex.php?latex=y&#038;bg=ffffff&#038;fg=111111&#038;s=0&#038;c=20201002" alt="y" title="y" class="latex" />. The likelihood is given by Eq. (1). We then optimize Eq. (4) over the implicit variational distribution <img src="https://s0.wp.com/latex.php?latex=Q&#038;bg=ffffff&#038;fg=111111&#038;s=0&#038;c=20201002" alt="Q" title="Q" class="latex" /> formed by the multiplicative Gaussian output noise.</p>
<p>For our model of multiplicative output noising, <img src="https://s0.wp.com/latex.php?latex=Q&#038;bg=ffffff&#038;fg=111111&#038;s=0&#038;c=20201002" alt="Q" title="Q" class="latex" /> has two parameters we can try to fit: We have the vector of coefficients <img src="https://s0.wp.com/latex.php?latex=%5Calpha&#038;bg=ffffff&#038;fg=111111&#038;s=0&#038;c=20201002" alt="&#92;alpha" title="&#92;alpha" class="latex" />, and we have the multiplicative noise standard deviation <img src="https://s0.wp.com/latex.php?latex=%5Csigma&#038;bg=ffffff&#038;fg=111111&#038;s=0&#038;c=20201002" alt="&#92;sigma" title="&#92;sigma" class="latex" />. The exact posterior has the parameter <img src="https://s0.wp.com/latex.php?latex=%5Calpha&#038;bg=ffffff&#038;fg=111111&#038;s=0&#038;c=20201002" alt="&#92;alpha" title="&#92;alpha" class="latex" />, but <img src="https://s0.wp.com/latex.php?latex=%5Csigma&#038;bg=ffffff&#038;fg=111111&#038;s=0&#038;c=20201002" alt="&#92;sigma" title="&#92;sigma" class="latex" /> is only a parameter of <img src="https://s0.wp.com/latex.php?latex=Q&#038;bg=ffffff&#038;fg=111111&#038;s=0&#038;c=20201002" alt="Q" title="Q" class="latex" />. Expected likelihood is given by Eq. (1).</p>
<p>The likelihood term is not analytically tractable. Here is where the magic of stochastic variational inference comes in. The expected likelihood <img src="https://s0.wp.com/latex.php?latex=%5Cmathbb%7BE%7D_Q+%5Cleft%5B+%5Clog+H%28z%7Cx%29%5Cright%5D&#038;bg=ffffff&#038;fg=111111&#038;s=0&#038;c=20201002" alt="&#92;mathbb{E}_Q &#92;left[ &#92;log H(z|x)&#92;right]" title="&#92;mathbb{E}_Q &#92;left[ &#92;log H(z|x)&#92;right]" class="latex" /> can be found by sampling from <img src="https://s0.wp.com/latex.php?latex=Q%28Z%29&#038;bg=ffffff&#038;fg=111111&#038;s=0&#038;c=20201002" alt="Q(Z)" title="Q(Z)" class="latex" />! In our case, this is done by just explicitly multiplicatively noising the output of or model. This is very easy. It allows you to turn a frequentist model into a variational Bayesian model in a few keystrokes. It sure beats MCMC. While we never get an analytical posterior distribution, we can easily sample from it.</p>
<p>This KL divergence <img src="https://s0.wp.com/latex.php?latex=%5Cmathrm%7BD_%7BKL%7D%7D%28Q%7C%7CF%29&#038;bg=ffffff&#038;fg=111111&#038;s=0&#038;c=20201002" alt="&#92;mathrm{D_{KL}}(Q||F)" title="&#92;mathrm{D_{KL}}(Q||F)" class="latex" /> does not have a tractable form either. It depends only on <img src="https://s0.wp.com/latex.php?latex=%5Csigma&#038;bg=ffffff&#038;fg=111111&#038;s=0&#038;c=20201002" alt="&#92;sigma" title="&#92;sigma" class="latex" />, not on the coefficients <img src="https://s0.wp.com/latex.php?latex=%5Calpha&#038;bg=ffffff&#038;fg=111111&#038;s=0&#038;c=20201002" alt="&#92;alpha" title="&#92;alpha" class="latex" />. If we fix the multiplicative noise level <img src="https://s0.wp.com/latex.php?latex=%5Csigma&#038;bg=ffffff&#038;fg=111111&#038;s=0&#038;c=20201002" alt="&#92;sigma" title="&#92;sigma" class="latex" />, this term can be ignored, since it is not affected by <img src="https://s0.wp.com/latex.php?latex=%5Calpha&#038;bg=ffffff&#038;fg=111111&#038;s=0&#038;c=20201002" alt="&#92;alpha" title="&#92;alpha" class="latex" />. It can be formed by sampling, but <a href="https://arxiv.org/abs/1701.05369">Molchanov 2017</a> came up with a very accurate analytical approximation. Unfortunately, I got pretty inconsistent results allowing for <img src="https://s0.wp.com/latex.php?latex=%5Csigma&#038;bg=ffffff&#038;fg=111111&#038;s=0&#038;c=20201002" alt="&#92;sigma" title="&#92;sigma" class="latex" /> to adapt. Therefore, here I keep the noise level fixed, meaning that we can ignore the KL term and just fit the variational distribution based on expected log-likelihood, given by Eq. (1).</p>
<h2 id="Multiplicative-output-noising,-L2-regularization,-and-early-stopping">Multiplicative output noising, L2 regularization, and early stopping</h2>
<p>I used Adagrad gradient descent to train these models. I found that adding no regularization tended to produce results almost as good as using dropout, except with an order of magnitude less iterations. Even with very noisy datasets, I wasn&#8217;t getting much in the way of overfitting, despite the fact that unregularized kernel models will exactly fit the data at the optimal solution. A big disadvantage of dropout is that it produces longer training times, since it adds variance to the model. When using it for a kernel method on datasets with not too complicated decision boundaries, you need a really big dropout rate to get any kind of effective regularization. That makes the variance problem worse.</p>
<p>To illustrate this, I&#8217;ll fit the &#8220;circles&#8221; synthetic data from Scikit-Learn. This gives synthetic data for a binary classification problem where the points belonging to each class lie on two concentric circles. Here, I generate 1000 samples, with added noise, causing the circles to overlap. I fit three different models to these data: First, Gaussian process classification with no dropout or regularization, trained with Adagrad for 500 epochs. Second, a Gaussian process classification model with dropout trained with Adagrad to 5000 epochs. Third, Scikit-Learn&#8217;s Gaussian process classification model with hyperparameter adaptation. 10,000 different points were generated as test data. The first two models were written using Tensorflow.</p>
<div class="cell border-box-sizing code_cell rendered">
<div class="input">
<div class="inner_cell">
<div class="input_area">
<div class=" highlight hl-ipython2">
<pre><span class="kn">from</span> <span class="nn">variational_glm</span> <span class="kn">import</span> <span class="o">*</span>
<span class="kn">from</span> <span class="nn">sklearn</span> <span class="kn">import</span> <span class="n">datasets</span>
<span class="kn">from</span> <span class="nn">tensorflow.examples.tutorials.mnist</span> <span class="kn">import</span> <span class="n">input_data</span>
<span class="kn">from</span> <span class="nn">sklearn.linear_model</span> <span class="kn">import</span> <span class="n">SGDClassifier</span>
<span class="kn">from</span> <span class="nn">sklearn.multiclass</span> <span class="kn">import</span> <span class="n">OneVsRestClassifier</span>
<span class="kn">from</span> <span class="nn">sklearn.svm</span> <span class="kn">import</span> <span class="n">LinearSVC</span><span class="p">,</span> <span class="n">SVC</span>
<span class="kn">from</span> <span class="nn">sklearn.datasets</span> <span class="kn">import</span> <span class="n">make_circles</span>
<span class="kn">from</span> <span class="nn">sklearn.metrics.pairwise</span> <span class="kn">import</span> <span class="n">rbf_kernel</span>
<span class="kn">import</span> <span class="nn">matplotlib.pyplot</span> <span class="kn">as</span> <span class="nn">plt</span>
<span class="kn">import</span> <span class="nn">scipy.linalg</span> <span class="kn">as</span> <span class="nn">la</span>
<span class="kn">import</span> <span class="nn">sklearn.gaussian_process</span> <span class="kn">as</span> <span class="nn">gp</span>

<span class="c1">#matplotlib as weird defaults.</span>
<span class="n">plt</span><span class="o">.</span><span class="n">style</span><span class="o">.</span><span class="n">use</span><span class="p">(</span><span class="s1">'ggplot'</span><span class="p">)</span>
<span class="n">n_points</span><span class="o">=</span><span class="mi">10</span><span class="o">**</span><span class="mi">3</span>
<span class="n">n_test</span> <span class="o">=</span> <span class="mi">10</span><span class="o">**</span><span class="mi">4</span>
<span class="n">gamma</span><span class="o">=</span><span class="mi">100</span>
<span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">seed</span><span class="p">(</span><span class="mi">0</span><span class="p">)</span>

<span class="c1">#two models for gradient-descent trained GPR.</span>
<span class="c1">#Trying to keep to sklearn API, other parameters are in</span>
<span class="c1">#"fit" method.</span>
<span class="n">mod_nodrop</span> <span class="o">=</span> <span class="n">LinearModel</span><span class="p">(</span><span class="n">logistic_loss</span><span class="p">)</span>
<span class="n">mod</span> <span class="o">=</span> <span class="n">LinearModel</span><span class="p">(</span><span class="n">logistic_loss</span><span class="p">)</span>

<span class="c1">#generate train and test data</span>
<span class="n">circles</span> <span class="o">=</span> <span class="n">make_circles</span><span class="p">(</span><span class="n">n_points</span><span class="p">,</span><span class="n">noise</span><span class="o">=</span><span class="mf">0.1</span><span class="p">)</span>
<span class="n">test_circles</span> <span class="o">=</span> <span class="n">make_circles</span><span class="p">(</span><span class="n">n_test</span><span class="p">,</span> <span class="n">noise</span><span class="o">=</span><span class="mf">0.1</span><span class="p">)</span>

<span class="c1">#get the kernel matrices for train and test data.</span>
<span class="n">K</span> <span class="o">=</span> <span class="n">rbf_kernel</span><span class="p">(</span><span class="n">circles</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span><span class="n">circles</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span><span class="n">gamma</span><span class="o">=</span><span class="n">gamma</span><span class="p">)</span> 
<span class="n">Y</span> <span class="o">=</span> <span class="n">circles</span><span class="p">[</span><span class="mi">1</span><span class="p">][:,</span><span class="n">np</span><span class="o">.</span><span class="n">newaxis</span><span class="p">]</span>
<span class="n">K_test</span> <span class="o">=</span> <span class="n">rbf_kernel</span><span class="p">(</span><span class="n">test_circles</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span><span class="n">circles</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span><span class="n">gamma</span><span class="o">=</span><span class="n">gamma</span><span class="p">)</span>
<span class="n">Y_test</span> <span class="o">=</span> <span class="n">test_circles</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span>

<span class="n">mod_nodrop</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">K</span><span class="p">,</span><span class="n">Y</span><span class="p">,</span><span class="n">learning_rate</span><span class="o">=</span><span class="mf">1e-6</span><span class="p">,</span><span class="n">n_epochs</span><span class="o">=</span><span class="mi">500</span><span class="p">,</span><span class="n">batch_size</span><span class="o">=</span><span class="mi">50</span><span class="p">,</span>\
               <span class="n">init_stddev</span><span class="o">=</span><span class="mf">1e-6</span><span class="p">,</span><span class="n">use_dropout</span><span class="o">=</span><span class="bp">False</span><span class="p">,</span><span class="n">l2_penalty</span><span class="o">=</span><span class="mf">0.</span><span class="p">)</span>
<span class="n">mod</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">K</span><span class="p">,</span><span class="n">Y</span><span class="p">,</span><span class="n">learning_rate</span><span class="o">=</span><span class="mf">1e-6</span><span class="p">,</span><span class="n">n_epochs</span><span class="o">=</span><span class="mi">5000</span><span class="p">,</span><span class="n">batch_size</span><span class="o">=</span><span class="mi">50</span><span class="p">,</span>\
        <span class="n">init_stddev</span><span class="o">=</span><span class="mf">1e-6</span><span class="p">,</span><span class="n">use_dropout</span><span class="o">=</span><span class="bp">True</span><span class="p">,</span><span class="n">l2_penalty</span><span class="o">=</span><span class="mf">0.</span><span class="p">,</span><span class="n">dropout_stddev</span><span class="o">=</span><span class="mf">1e0</span><span class="p">)</span>
</pre>
</div>
</div>
</div>
</div>
<div class="output_wrapper"></div>
</div>
<div class="cell border-box-sizing code_cell rendered">
<div class="input">
<div class="inner_cell">
<div class="input_area">
<div class=" highlight hl-ipython2">
<pre><span class="c1">#plot the circles test data.</span>
<span class="n">plt</span><span class="o">.</span><span class="n">scatter</span><span class="p">(</span><span class="n">circles</span><span class="p">[</span><span class="mi">0</span><span class="p">][:,</span><span class="mi">0</span><span class="p">],</span><span class="n">circles</span><span class="p">[</span><span class="mi">0</span><span class="p">][:,</span><span class="mi">1</span><span class="p">],</span><span class="n">c</span><span class="o">=</span><span class="n">Y</span><span class="p">,</span><span class="n">edgecolors</span><span class="o">=</span><span class="bp">None</span><span class="p">)</span>                                                                                                                                                               
<span class="n">plt</span><span class="o">.</span><span class="n">title</span><span class="p">(</span><span class="s2">"Circles training data"</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">show</span><span class="p">()</span>
<span class="c1">#predictions with dropout</span>
<span class="n">tf_preds</span> <span class="o">=</span> <span class="n">mod</span><span class="o">.</span><span class="n">predict</span><span class="p">(</span><span class="n">K_test</span><span class="p">)[:,</span><span class="mi">0</span><span class="p">]</span>
<span class="c1">#predictions without dropout</span>
<span class="n">tf_preds_nodrop</span> <span class="o">=</span> <span class="n">mod_nodrop</span><span class="o">.</span><span class="n">predict</span><span class="p">(</span><span class="n">K_test</span><span class="p">)[:,</span><span class="mi">0</span><span class="p">]</span>

<span class="n">corrects</span> <span class="o">=</span> <span class="p">((</span><span class="n">tf_preds</span> <span class="o">&gt;</span><span class="mi">0</span><span class="p">)</span> <span class="o">==</span> <span class="n">test_circles</span><span class="p">[</span><span class="mi">1</span><span class="p">])</span>
<span class="n">corrects_nodrop</span> <span class="o">=</span> <span class="p">((</span><span class="n">tf_preds_nodrop</span> <span class="o">&gt;</span><span class="mi">0</span><span class="p">)</span><span class="o">==</span> <span class="n">test_circles</span><span class="p">[</span><span class="mi">1</span><span class="p">])</span>



<span class="c1">#kernel is squared exponential and white noise kernel (L2 regularization)</span>
<span class="n">kern</span> <span class="o">=</span> <span class="n">gp</span><span class="o">.</span><span class="n">kernels</span><span class="o">.</span><span class="n">RBF</span><span class="p">(</span><span class="n">length_scale</span><span class="o">=</span><span class="n">gamma</span><span class="p">)</span><span class="o">+</span><span class="n">gp</span><span class="o">.</span><span class="n">kernels</span><span class="o">.</span><span class="n">WhiteKernel</span><span class="p">()</span>
<span class="n">g</span> <span class="o">=</span> <span class="n">gp</span><span class="o">.</span><span class="n">GaussianProcessClassifier</span><span class="p">(</span><span class="n">kernel</span><span class="o">=</span><span class="n">kern</span><span class="p">,</span><span class="n">n_restarts_optimizer</span><span class="o">=</span><span class="mi">2</span><span class="p">)</span>
<span class="n">g</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">circles</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span><span class="n">circles</span><span class="p">[</span><span class="mi">1</span><span class="p">])</span>
<span class="n">preds_gpr</span> <span class="o">=</span> <span class="n">g</span><span class="o">.</span><span class="n">predict</span><span class="p">(</span><span class="n">test_circles</span><span class="p">[</span><span class="mi">0</span><span class="p">])</span>

<span class="k">print</span> <span class="s2">"Accuracy with dropout:"</span><span class="p">,</span> <span class="n">np</span><span class="o">.</span><span class="n">mean</span><span class="p">(</span><span class="n">corrects</span><span class="p">)</span>
<span class="k">print</span> <span class="s2">"Accuracy without dropout:"</span><span class="p">,</span> <span class="n">corrects_nodrop</span><span class="o">.</span><span class="n">mean</span><span class="p">()</span>
<span class="k">print</span> <span class="s2">"Accuracy of sklearn GPC:"</span><span class="p">,</span> <span class="p">(</span><span class="n">test_circles</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span><span class="o">==</span><span class="n">preds_gpr</span><span class="p">)</span><span class="o">.</span><span class="n">mean</span><span class="p">()</span>
</pre>
</div>
</div>
</div>
</div>
<div class="output_wrapper">
<div class="output">
<div class="output_area">
<div class="prompt"></div>
<div class="output_png output_subarea "><img /></div>
</div>
<div class="output_area">
<div class="prompt"></div>
<div class="output_subarea output_stream output_stdout output_text">
<pre>Accuracy with dropout: 0.8358
Accuracy without dropout: 0.8337
Accuracy of sklearn GPC: 0.8418
</pre>
</div>
</div>
</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered">
<div class="prompt input_prompt"></div>
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>From this, we can see that dropout with 5000 epochs gives only slightly superior performance to no dropout with almost no regularization, and only 500 epochs. The result without regularization does not overfit. Running it for 5000 iterations begins to degrade the performance, causing some overfitting, but not much. Keep in mind that the optimum solution for this kernel would <em>exactly fit</em> every training point, which would be enormously overfitting this dataset.</p>
<p>The fact that this model is not overfitting, despite being such a noisy dataset, must be due to the learning algorithm rather than the objective function. In most circumstances, kernel matrices are positive definite (i.e., they are nonsingular, with all positive eigenvalues). However, without regularization, they are usually numerically singular or close to it. This is because <strong>the eigenvalues of kernel matrices usually decay exponentially</strong>. There&#8217;s a few big eigenvalues, but a lot of really small ones. This causes the problem to become ill-posed, and the parts of the solution corresponding to small eigenvalues converge slowly.</p>
<p>Kernel approximation methods like Nystroem or Random Kitchen Sinks exploit this fact in creating low-rank kernel matrix approximations. You can usually approximate the biggest, most important eigenvalues with a low-rank approximation quite well.</p>
<p>Sadly, the Sklearn gaussian process classification model beats dropout. It does optimize hyperparameters, which can help. However, I think the big difference is that even using Adagrad, it is too difficult to fit the parts of the objective corresponding to small eigenvalues. Sklearn uses second-order accurate L-BGFS, which can deal with this a lot better.</p>
<p>We can take an eigendecomposition of <img src="https://s0.wp.com/latex.php?latex=%5Cmathbf%7BK%7D&#038;bg=ffffff&#038;fg=111111&#038;s=0&#038;c=20201002" alt="&#92;mathbf{K}" title="&#92;mathbf{K}" class="latex" /> to look at its spectrum. Below, I plot the log10 of the spectrum.</p>
</div>
</div>
</div>
<div class="cell border-box-sizing code_cell rendered">
<div class="input">
<div class="prompt input_prompt"></div>
<div class="inner_cell">
<div class="input_area">
<div class=" highlight hl-ipython2">
<pre><span class="c1">#Put in a tiny diagonal component (like a tiny amount of L2 regularization) </span>
<span class="c1">#so the eigenvalue decomposition is numerically stable.</span>
<span class="n">K_eigs</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">sort</span><span class="p">(</span><span class="n">la</span><span class="o">.</span><span class="n">eigvals</span><span class="p">(</span><span class="n">K</span> <span class="o">+</span> <span class="mf">1e-8</span><span class="o">*</span><span class="n">np</span><span class="o">.</span><span class="n">eye</span><span class="p">(</span><span class="n">n_points</span><span class="p">)))</span>
<span class="n">plt</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">log10</span><span class="p">(</span><span class="n">K_eigs</span><span class="p">))</span>
<span class="n">plt</span><span class="o">.</span><span class="n">xlabel</span><span class="p">(</span><span class="s2">"Eigenvalue number"</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">ylabel</span><span class="p">(</span><span class="s2">"Log_10 eigenvalue"</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">show</span><span class="p">()</span>

<span class="k">print</span> <span class="s2">"Log_10 condition number:"</span><span class="p">,</span> <span class="n">np</span><span class="o">.</span><span class="n">log10</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">linalg</span><span class="o">.</span><span class="n">cond</span><span class="p">(</span><span class="n">K</span><span class="p">))</span>
</pre>
</div>
</div>
</div>
</div>
<div class="output_wrapper">
<div class="output">
<div class="output_area">
<div class="prompt"></div>
<div class="output_png output_subarea "><img /></div>
</div>
<div class="output_area">
<div class="prompt"></div>
<div class="output_subarea output_stream output_stdout output_text">
<pre>Log_10 condition number: 10.6476310408
</pre>
</div>
</div>
</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered">
<div class="prompt input_prompt"></div>
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>In this plot, eigenvalue 1000 is the largest, and 0 the smallest. Except for the smallest log eigenvalues, this is nearly linear. This confirms that the eigenvalues drop off exponentially. This also reveals an additional reason that kernel methods are almost invariably used with regularization: this exponential decay makes for very high condition numbers, about <img src="https://s0.wp.com/latex.php?latex=10%5E%7B11%7D&#038;bg=ffffff&#038;fg=111111&#038;s=0&#038;c=20201002" alt="10^{11}" title="10^{11}" class="latex" /> in this case. Direct linear solvers like Cholesky decomposition lose a ton of accuracy, and iterative methods like conjugate gradient simply won&#8217;t work (at least without a really goodu preconditioner). Adding L2 regularization is equivalent to adding a diagonal matrix to <img src="https://s0.wp.com/latex.php?latex=%5Cmathbf%7BK%7D&#038;bg=ffffff&#038;fg=111111&#038;s=0&#038;c=20201002" alt="&#92;mathbf{K}" title="&#92;mathbf{K}" class="latex" />, which embiggens the very small eigenvalues, and can improve the condition number dramatically.</p>
</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered">
<div class="prompt input_prompt"></div>
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<h2 id="$\mathbf{K}$,-its-eigenvalues,-and-its-condition-number">The kernel matrix, its eigenvalues, and its condition number</h2>
<p>Let&#8217;s examine the kernel matrix <img src="https://s0.wp.com/latex.php?latex=%5Cmathbf%7BK%7D&#038;bg=ffffff&#038;fg=111111&#038;s=0&#038;c=20201002" alt="&#92;mathbf{K}" title="&#92;mathbf{K}" class="latex" /> some more. Remember that <img src="https://s0.wp.com/latex.php?latex=K_%7Bij%7D+%3D+g%28%5Cmathbf%7Bx%7D_i%2C+%5Cmathbf%7Bx%7D_j%29&#038;bg=ffffff&#038;fg=111111&#038;s=0&#038;c=20201002" alt="K_{ij} = g(&#92;mathbf{x}_i, &#92;mathbf{x}_j)" title="K_{ij} = g(&#92;mathbf{x}_i, &#92;mathbf{x}_j)" class="latex" />, for data points <img src="https://s0.wp.com/latex.php?latex=%5Cmathbf%7Bx%7D_i&#038;bg=ffffff&#038;fg=111111&#038;s=0&#038;c=20201002" alt="&#92;mathbf{x}_i" title="&#92;mathbf{x}_i" class="latex" /> and <img src="https://s0.wp.com/latex.php?latex=%5Cmathbf%7Bx%7D_j&#038;bg=ffffff&#038;fg=111111&#038;s=0&#038;c=20201002" alt="&#92;mathbf{x}_j" title="&#92;mathbf{x}_j" class="latex" />, and a kernel function <img src="https://s0.wp.com/latex.php?latex=g&#038;bg=ffffff&#038;fg=111111&#038;s=0&#038;c=20201002" alt="g" title="g" class="latex" /> (we are using the squared-exponential kernel <img src="https://s0.wp.com/latex.php?latex=g%3D%5Cexp%28-%5Cgamma+%7C%7C%5Cmathbf%7Bx%7D_i-%5Cmathbf%7Bx%7D_j%7C%7C%5E2%29&#038;bg=ffffff&#038;fg=111111&#038;s=0&#038;c=20201002" alt="g=&#92;exp(-&#92;gamma ||&#92;mathbf{x}_i-&#92;mathbf{x}_j||^2)" title="g=&#92;exp(-&#92;gamma ||&#92;mathbf{x}_i-&#92;mathbf{x}_j||^2)" class="latex" /> here). For Gaussian processes, the kernel function gives the covariance between any two points, and <img src="https://s0.wp.com/latex.php?latex=%5Cmathbf%7BK%7D&#038;bg=ffffff&#038;fg=111111&#038;s=0&#038;c=20201002" alt="&#92;mathbf{K}" title="&#92;mathbf{K}" class="latex" /> is the covariance matrix for a finite set of points <img src="https://s0.wp.com/latex.php?latex=%5Cmathbf%7Bx%7D_1%2C...%5Cmathbf%7Bx%7D_n&#038;bg=ffffff&#038;fg=111111&#038;s=0&#038;c=20201002" alt="&#92;mathbf{x}_1,...&#92;mathbf{x}_n" title="&#92;mathbf{x}_1,...&#92;mathbf{x}_n" class="latex" />.</p>
<p>For unregularized kernel regression with squared loss (I&#8217;m keeping it simple here), with data vector <img src="https://s0.wp.com/latex.php?latex=%5Cmathbf%7By%7D&#038;bg=ffffff&#038;fg=111111&#038;s=0&#038;c=20201002" alt="&#92;mathbf{y}" title="&#92;mathbf{y}" class="latex" /> and weights <img src="https://s0.wp.com/latex.php?latex=%5Cmathbf%7Bw%7D&#038;bg=ffffff&#038;fg=111111&#038;s=0&#038;c=20201002" alt="&#92;mathbf{w}" title="&#92;mathbf{w}" class="latex" />, we need to solve the system,</p>
<p><img src="https://s0.wp.com/latex.php?latex=%5Cmathbf%7BK+w%7D+%3D+%5Cmathbf%7By%7D&#038;bg=ffffff&#038;fg=111111&#038;s=0&#038;c=20201002" alt="&#92;mathbf{K w} = &#92;mathbf{y}" title="&#92;mathbf{K w} = &#92;mathbf{y}" class="latex" />.</p>
<p>Since <img src="https://s0.wp.com/latex.php?latex=%5Cmathbf%7BK%7D&#038;bg=ffffff&#038;fg=111111&#038;s=0&#038;c=20201002" alt="&#92;mathbf{K}" title="&#92;mathbf{K}" class="latex" /> is positive definite (or at least positive semi-definite), we can do this eigenvalue decomposition:</p>
<p><img src="https://s0.wp.com/latex.php?latex=%5Cmathbf%7BK%7D+%3D+%5Cmathbf%7BQ+%5CLambda+Q%5ET%7D%2C&#038;bg=ffffff&#038;fg=111111&#038;s=0&#038;c=20201002" alt="&#92;mathbf{K} = &#92;mathbf{Q &#92;Lambda Q^T}," title="&#92;mathbf{K} = &#92;mathbf{Q &#92;Lambda Q^T}," class="latex" /></p>
<p>where <img src="https://s0.wp.com/latex.php?latex=%5Cmathbf%7BQ%7D&#038;bg=ffffff&#038;fg=111111&#038;s=0&#038;c=20201002" alt="&#92;mathbf{Q}" title="&#92;mathbf{Q}" class="latex" /> are the orthogonal eigenvectors and <img src="https://s0.wp.com/latex.php?latex=%5Cmathbf%7B%5CLambda%7D&#038;bg=ffffff&#038;fg=111111&#038;s=0&#038;c=20201002" alt="&#92;mathbf{&#92;Lambda}" title="&#92;mathbf{&#92;Lambda}" class="latex" /> is a diagonal matrix formed by the eigenvalues. We can multiply both sides on the left by <img src="https://s0.wp.com/latex.php?latex=%5Cmathbf%7BQ%5ET%7D&#038;bg=ffffff&#038;fg=111111&#038;s=0&#038;c=20201002" alt="&#92;mathbf{Q^T}" title="&#92;mathbf{Q^T}" class="latex" /> to yield the system,</p>
<p><img src="https://s0.wp.com/latex.php?latex=%5Cmathbf%7B%5CLambda+Q%5ET+w%7D+%3D+%5Cmathbf%7BQ%5ET+y%7D&#038;bg=ffffff&#038;fg=111111&#038;s=0&#038;c=20201002" alt="&#92;mathbf{&#92;Lambda Q^T w} = &#92;mathbf{Q^T y}" title="&#92;mathbf{&#92;Lambda Q^T w} = &#92;mathbf{Q^T y}" class="latex" /></p>
<p>or,</p>
<p><img src="https://s0.wp.com/latex.php?latex=%5Cmathbf%7B%5CLambda+w%5E%2A%7D+%3D+%5Cmathbf%7Bw%5E%2A%7D%2C&#038;bg=ffffff&#038;fg=111111&#038;s=0&#038;c=20201002" alt="&#92;mathbf{&#92;Lambda w^*} = &#92;mathbf{w^*}," title="&#92;mathbf{&#92;Lambda w^*} = &#92;mathbf{w^*}," class="latex" /></p>
<p>where <img src="https://s0.wp.com/latex.php?latex=%5Cmathbf%7Bw%5E%2A%7D&#038;bg=ffffff&#038;fg=111111&#038;s=0&#038;c=20201002" alt="&#92;mathbf{w^*}" title="&#92;mathbf{w^*}" class="latex" /> and <img src="https://s0.wp.com/latex.php?latex=%5Cmathbf%7By%5E%2A%7D&#038;bg=ffffff&#038;fg=111111&#038;s=0&#038;c=20201002" alt="&#92;mathbf{y^*}" title="&#92;mathbf{y^*}" class="latex" /> are respectively <img src="https://s0.wp.com/latex.php?latex=%5Cmathbf%7Bw%7D&#038;bg=ffffff&#038;fg=111111&#038;s=0&#038;c=20201002" alt="&#92;mathbf{w}" title="&#92;mathbf{w}" class="latex" /> and <img src="https://s0.wp.com/latex.php?latex=%5Cmathbf%7By%7D&#038;bg=ffffff&#038;fg=111111&#038;s=0&#038;c=20201002" alt="&#92;mathbf{y}" title="&#92;mathbf{y}" class="latex" /> transformed by <img src="https://s0.wp.com/latex.php?latex=%5Cmathbf%7BQ%5ET%7D&#038;bg=ffffff&#038;fg=111111&#038;s=0&#038;c=20201002" alt="&#92;mathbf{Q^T}" title="&#92;mathbf{Q^T}" class="latex" />. Since <img src="https://s0.wp.com/latex.php?latex=%5Cmathbf%7B%5CLambda%7D&#038;bg=ffffff&#038;fg=111111&#038;s=0&#038;c=20201002" alt="&#92;mathbf{&#92;Lambda}" title="&#92;mathbf{&#92;Lambda}" class="latex" /> is diagonal, the solution for an arbitrary weight <img src="https://s0.wp.com/latex.php?latex=w_i&#038;bg=ffffff&#038;fg=111111&#038;s=0&#038;c=20201002" alt="w_i" title="w_i" class="latex" /> is,</p>
<p><img src="https://s0.wp.com/latex.php?latex=w_i+%3D+%5Cfrac%7By_i%5E%2A%7D%7B%5Clambda_i%7D&#038;bg=ffffff&#038;fg=111111&#038;s=0&#038;c=20201002" alt="w_i = &#92;frac{y_i^*}{&#92;lambda_i}" title="w_i = &#92;frac{y_i^*}{&#92;lambda_i}" class="latex" /></p>
<p>If <img src="https://s0.wp.com/latex.php?latex=%5Clambda_i&#038;bg=ffffff&#038;fg=111111&#038;s=0&#038;c=20201002" alt="&#92;lambda_i" title="&#92;lambda_i" class="latex" /> is really small, then <img src="https://s0.wp.com/latex.php?latex=w_i%5E%2A&#038;bg=ffffff&#038;fg=111111&#038;s=0&#038;c=20201002" alt="w_i^*" title="w_i^*" class="latex" /> is really sensitive to measurement noise in <img src="https://s0.wp.com/latex.php?latex=y_i%5E%2A&#038;bg=ffffff&#038;fg=111111&#038;s=0&#038;c=20201002" alt="y_i^*" title="y_i^*" class="latex" />. Also, because floating point numbers aren&#8217;t actually numbers, it becomes increasingly numerically unstable. So, it&#8217;s necessary to do some kind of modification to the kernel matrix to ease this problem. An obvious fix is the standard L2 regularization: Add a diagonal matrix <img src="https://s0.wp.com/latex.php?latex=%5Cmathbf%7B%5CXi%7D&#038;bg=ffffff&#038;fg=111111&#038;s=0&#038;c=20201002" alt="&#92;mathbf{&#92;Xi}" title="&#92;mathbf{&#92;Xi}" class="latex" /> to <img src="https://s0.wp.com/latex.php?latex=%5Cmathbf%7BK%7D&#038;bg=ffffff&#038;fg=111111&#038;s=0&#038;c=20201002" alt="&#92;mathbf{K}" title="&#92;mathbf{K}" class="latex" />. Then we are trying to solve the system,</p>
<p><img src="https://s0.wp.com/latex.php?latex=%28%5Cmathbf%7BK%7D+%2B+%5Cmathbf%7B%5CXi%7D%29%5Cmathbf%7Bw%7D+%3D+%5Cmathbf%7By%7D&#038;bg=ffffff&#038;fg=111111&#038;s=0&#038;c=20201002" alt="(&#92;mathbf{K} + &#92;mathbf{&#92;Xi})&#92;mathbf{w} = &#92;mathbf{y}" title="(&#92;mathbf{K} + &#92;mathbf{&#92;Xi})&#92;mathbf{w} = &#92;mathbf{y}" class="latex" /></p>
<p>If we diagonalize this just like before, we wind up with,</p>
<p><img src="https://s0.wp.com/latex.php?latex=%5Cmathbf%7B%28%5CLambda+%2B+%5CXi%29+w%5E%2A%7D+%3D+%5Cmathbf%7BQ%5ET+y%5E%2A%7D&#038;bg=ffffff&#038;fg=111111&#038;s=0&#038;c=20201002" alt="&#92;mathbf{(&#92;Lambda + &#92;Xi) w^*} = &#92;mathbf{Q^T y^*}" title="&#92;mathbf{(&#92;Lambda + &#92;Xi) w^*} = &#92;mathbf{Q^T y^*}" class="latex" /></p>
<p>Note that now, an eigenvalue of this system <img src="https://s0.wp.com/latex.php?latex=%5Clambda_i+%2B+%5Cxi_i&#038;bg=ffffff&#038;fg=111111&#038;s=0&#038;c=20201002" alt="&#92;lambda_i + &#92;xi_i" title="&#92;lambda_i + &#92;xi_i" class="latex" />, where <img src="https://s0.wp.com/latex.php?latex=%5Cxi_i&#038;bg=ffffff&#038;fg=111111&#038;s=0&#038;c=20201002" alt="&#92;xi_i" title="&#92;xi_i" class="latex" /> is the <img src="https://s0.wp.com/latex.php?latex=i&#038;bg=ffffff&#038;fg=111111&#038;s=0&#038;c=20201002" alt="i" title="i" class="latex" /> diagonal component of <img src="https://s0.wp.com/latex.php?latex=%5CXi&#038;bg=ffffff&#038;fg=111111&#038;s=0&#038;c=20201002" alt="&#92;Xi" title="&#92;Xi" class="latex" />. This bounds the eigenvalue from below by <img src="https://s0.wp.com/latex.php?latex=%5Cxi_i&#038;bg=ffffff&#038;fg=111111&#038;s=0&#038;c=20201002" alt="&#92;xi_i" title="&#92;xi_i" class="latex" />. The solution for <img src="https://s0.wp.com/latex.php?latex=w%5E%2A_i&#038;bg=ffffff&#038;fg=111111&#038;s=0&#038;c=20201002" alt="w^*_i" title="w^*_i" class="latex" /> is,</p>
<p><img src="https://s0.wp.com/latex.php?latex=w_i%5E%2A+%3D+%5Cfrac%7By_i%5E%2A%7D%7B%5Clambda_i+%2B+%5Cxi_i%7D&#038;bg=ffffff&#038;fg=111111&#038;s=0&#038;c=20201002" alt="w_i^* = &#92;frac{y_i^*}{&#92;lambda_i + &#92;xi_i}" title="w_i^* = &#92;frac{y_i^*}{&#92;lambda_i + &#92;xi_i}" class="latex" /></p>
<p>So, we can see here that weights associated with small eigenvalues (<img src="https://s0.wp.com/latex.php?latex=%5Clambda_i+%5Cll+%5Cxi_i&#038;bg=ffffff&#038;fg=111111&#038;s=0&#038;c=20201002" alt="&#92;lambda_i &#92;ll &#92;xi_i" title="&#92;lambda_i &#92;ll &#92;xi_i" class="latex" />) approach <img src="https://s0.wp.com/latex.php?latex=y_i%5E%2A%2F%5Cxi_i&#038;bg=ffffff&#038;fg=111111&#038;s=0&#038;c=20201002" alt="y_i^*/&#92;xi_i" title="y_i^*/&#92;xi_i" class="latex" />, so weights can&#8217;t get too massive. Weights associated with small eigenvalues are damped a lot, weights associated with big eigenvalues aren&#8217;t damped as much (i.e., they are more faithful to the data).</p>
<p>Instead of thinking of this problem as solving a linear system, we can view it as an equivalent minimization problem. Again, in reference frame where the kernel matrix is diagonal, we can just write this as a bunch of independent scalar problems for simplicity:</p>
<p><img src="https://s0.wp.com/latex.php?latex=%5Cmin_%7Bw%5E%2A_i%7D%28%28%5Clambda_i+%2B+%5Cxi_i%29w%5E%2A_i+-+y%5E%7B%2AT%7D_i%29%5E2&#038;bg=ffffff&#038;fg=111111&#038;s=0&#038;c=20201002" alt="&#92;min_{w^*_i}((&#92;lambda_i + &#92;xi_i)w^*_i - y^{*T}_i)^2" title="&#92;min_{w^*_i}((&#92;lambda_i + &#92;xi_i)w^*_i - y^{*T}_i)^2" class="latex" /></p>
<p>The corresponding solution for a weight <img src="https://s0.wp.com/latex.php?latex=w_i%5E%2A&#038;bg=ffffff&#038;fg=111111&#038;s=0&#038;c=20201002" alt="w_i^*" title="w_i^*" class="latex" /> of the unregularized problem (with <img src="https://s0.wp.com/latex.php?latex=%5Cxi_i+%3D+0&#038;bg=ffffff&#038;fg=111111&#038;s=0&#038;c=20201002" alt="&#92;xi_i = 0" title="&#92;xi_i = 0" class="latex" />) is <img src="https://s0.wp.com/latex.php?latex=y%5E%2A_i%2F%5Clambda_i&#038;bg=ffffff&#038;fg=111111&#038;s=0&#038;c=20201002" alt="y^*_i/&#92;lambda_i" title="y^*_i/&#92;lambda_i" class="latex" />.</p>
</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered">
<div class="prompt input_prompt"></div>
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<h2 id="Dropout-as-multiplicative-noise,-and-as-additive-noise">Dropout as multiplicative noise, and as additive noise</h2>
</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered">
<div class="prompt input_prompt"></div>
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>Dropout is one of the most popular regularization techniques for neural networks. In the standard Bernoulli dropout, each neuron is turned of with some probability at each iteration. Another version instead adds <em>multiplicative</em> Gaussian noise to inputs of a layer. These two variants are pretty much the same, because repeated trials of the Bernoulli distribution (i.e. the binomial distribution) approach the normal distribution due to the central limit theorem. Since this noise only affects the objective function through the activation functions, we can instead add noise to the preactivations. In a linear model, this corresponds to multiplicative output noise (i.e. noise on <img src="https://s0.wp.com/latex.php?latex=%5Cmathbf%7B%5Chat%7By%7D%7D&#038;bg=ffffff&#038;fg=111111&#038;s=0&#038;c=20201002" alt="&#92;mathbf{&#92;hat{y}}" title="&#92;mathbf{&#92;hat{y}}" class="latex" />) instead of noise in the weights or the input dimensions. The output dimension is usually quite a bit smaller than the input, so this version is computationally easier. It&#8217;s also feasible to generate a sample for each example in every minibatch, which helps reduce variance of the gradient. Noising the inputs, or noising the outputs of a generalized Gaussian process model is a six-of-one, half a dozen of the other sort of situation.</p>
<p>Note that with the assumption of a squared-exponential kernel (or any kernel continuous at <img src="https://s0.wp.com/latex.php?latex=%5Cmathbf%7Bx%7D_i+%3D+%5Cmathbf%7Bx_j%7D&#038;bg=ffffff&#038;fg=111111&#038;s=0&#038;c=20201002" alt="&#92;mathbf{x}_i = &#92;mathbf{x_j}" title="&#92;mathbf{x}_i = &#92;mathbf{x_j}" class="latex" />), we are assuming there is no measurement noise in <img src="https://s0.wp.com/latex.php?latex=y%5E%2A_i&#038;bg=ffffff&#038;fg=111111&#038;s=0&#038;c=20201002" alt="y^*_i" title="y^*_i" class="latex" />. Measurement noise is usually thought of as something similar to Gaussian white noise. The noise of an individual observation shouldn&#8217;t be correlated with a spatially nearby observation. However, if the kernel is continous, then for any <img src="https://s0.wp.com/latex.php?latex=%5Cepsilon+%3E0&#038;bg=ffffff&#038;fg=111111&#038;s=0&#038;c=20201002" alt="&#92;epsilon &gt;0" title="&#92;epsilon &gt;0" class="latex" />, we can choose <img src="https://s0.wp.com/latex.php?latex=%5Cdelta&#038;bg=ffffff&#038;fg=111111&#038;s=0&#038;c=20201002" alt="&#92;delta" title="&#92;delta" class="latex" /> such that if <img src="https://s0.wp.com/latex.php?latex=%7C%7C%5Cmathbf%7Bx%7D_i+-+%5Cmathbf%7Bx%7D_j%7C%7C+%3C+%5Cdelta&#038;bg=ffffff&#038;fg=111111&#038;s=0&#038;c=20201002" alt="||&#92;mathbf{x}_i - &#92;mathbf{x}_j|| &lt; &#92;delta" title="||&#92;mathbf{x}_i - &#92;mathbf{x}_j|| &lt; &#92;delta" class="latex" />, then <img src="https://s0.wp.com/latex.php?latex=%5Cmathrm%7BCorr%7D%28%5Cmathbf%7Bx%7D_i%2C+%5Cmathbf%7Bx%7D_j%29+%3D+%5Cfrac%7B%5Cmathrm%7BCov%7D%28%5Cmathbf%7Bx%7D_i%2C+%5Cmathbf%7Bx%7D_j%29%7D%7B%5Csqrt%7B%5Cmathrm%7BCov%7D%28%5Cmathbf%7Bx%7D_i%2C+%5Cmathbf%7Bx%7D_i%29+%5Cmathrm%7BCov%7D%28%5Cmathbf%7Bx%7D_j%2C+%5Cmathbf%7Bx%7D_j%29%7D%7D+%3D+%5Cfrac%7B%5Cmathbf%7BK_%7Bij%7D%7D%7D%7B%5Csqrt%7B%5Cmathbf%7BK%7D_%7Bii%7D+%5Cmathbf%7BK%7D_%7Bjj%7D%7D%7D+%3E+1+-+%5Cepsilon&#038;bg=ffffff&#038;fg=111111&#038;s=0&#038;c=20201002" alt="&#92;mathrm{Corr}(&#92;mathbf{x}_i, &#92;mathbf{x}_j) = &#92;frac{&#92;mathrm{Cov}(&#92;mathbf{x}_i, &#92;mathbf{x}_j)}{&#92;sqrt{&#92;mathrm{Cov}(&#92;mathbf{x}_i, &#92;mathbf{x}_i) &#92;mathrm{Cov}(&#92;mathbf{x}_j, &#92;mathbf{x}_j)}} = &#92;frac{&#92;mathbf{K_{ij}}}{&#92;sqrt{&#92;mathbf{K}_{ii} &#92;mathbf{K}_{jj}}} &gt; 1 - &#92;epsilon" title="&#92;mathrm{Corr}(&#92;mathbf{x}_i, &#92;mathbf{x}_j) = &#92;frac{&#92;mathrm{Cov}(&#92;mathbf{x}_i, &#92;mathbf{x}_j)}{&#92;sqrt{&#92;mathrm{Cov}(&#92;mathbf{x}_i, &#92;mathbf{x}_i) &#92;mathrm{Cov}(&#92;mathbf{x}_j, &#92;mathbf{x}_j)}} = &#92;frac{&#92;mathbf{K_{ij}}}{&#92;sqrt{&#92;mathbf{K}_{ii} &#92;mathbf{K}_{jj}}} &gt; 1 - &#92;epsilon" class="latex" /></p>
<p>With these kind of kernels, if we have a realization of the Gaussian process at a particular point, then we know the value at sufficiently nearby points almost perfectly. Therefore, another way of arriving at L2 regularization, like in the previous section, is to add a white noise kernel to the squared-exponential kernel. The standard white noise kernel is defined as <img src="https://s0.wp.com/latex.php?latex=%5Ckappa%28%5Cmathbf%7Bx%7D_i%2C%5Cmathbf%7Bx%7D_j%29+%3D+%5Czeta&#038;bg=ffffff&#038;fg=111111&#038;s=0&#038;c=20201002" alt="&#92;kappa(&#92;mathbf{x}_i,&#92;mathbf{x}_j) = &#92;zeta" title="&#92;kappa(&#92;mathbf{x}_i,&#92;mathbf{x}_j) = &#92;zeta" class="latex" /> if <img src="https://s0.wp.com/latex.php?latex=%5Cmathbf%7Bx%7D_i+%3D+%5Cmathbf%7Bx%7D_%7Bj%7D&#038;bg=ffffff&#038;fg=111111&#038;s=0&#038;c=20201002" alt="&#92;mathbf{x}_i = &#92;mathbf{x}_{j}" title="&#92;mathbf{x}_i = &#92;mathbf{x}_{j}" class="latex" />, and <img src="https://s0.wp.com/latex.php?latex=%5Ckappa%28%5Cmathbf%7Bx%7D_i%2C%5Cmathbf%7Bx%7D_j%29%3D0&#038;bg=ffffff&#038;fg=111111&#038;s=0&#038;c=20201002" alt="&#92;kappa(&#92;mathbf{x}_i,&#92;mathbf{x}_j)=0" title="&#92;kappa(&#92;mathbf{x}_i,&#92;mathbf{x}_j)=0" class="latex" /> otherwise. It&#8217;s possible to make <img src="https://s0.wp.com/latex.php?latex=%5Czeta&#038;bg=ffffff&#038;fg=111111&#038;s=0&#038;c=20201002" alt="&#92;zeta" title="&#92;zeta" class="latex" /> spatially dependent, and give it a prior, etc.</p>
<p>Note that with Gaussian process models, by using the representer theorem the noise is viewed as being on <img src="https://s0.wp.com/latex.php?latex=%5Cmathbf%7By%7D&#038;bg=ffffff&#038;fg=111111&#038;s=0&#038;c=20201002" alt="&#92;mathbf{y}" title="&#92;mathbf{y}" class="latex" />, rather than on the weights as with Bayesian linear regression. I think this also makes way more intuitive sense. Who cares what noise on the weights is, or what the weights even are? We only care about the predictions <img src="https://s0.wp.com/latex.php?latex=%5Cmathbf%7B%5Chat%7By%7D%7D&#038;bg=ffffff&#038;fg=111111&#038;s=0&#038;c=20201002" alt="&#92;mathbf{&#92;hat{y}}" title="&#92;mathbf{&#92;hat{y}}" class="latex" />.</p>
<p>From before, remember that we are trying to minimize this objective:</p>
<p><img src="https://s0.wp.com/latex.php?latex=%5Cmathbb%7BE%7D_q%5B+L%28%5Cmathbf%7B%5Chat%7By%7D%7D%2C%5Cmathbf%7By%7D%29%5D&#038;bg=ffffff&#038;fg=111111&#038;s=0&#038;c=20201002" alt="&#92;mathbb{E}_q[ L(&#92;mathbf{&#92;hat{y}},&#92;mathbf{y})]" title="&#92;mathbb{E}_q[ L(&#92;mathbf{&#92;hat{y}},&#92;mathbf{y})]" class="latex" />,</p>
<p>where <img src="https://s0.wp.com/latex.php?latex=L&#038;bg=ffffff&#038;fg=111111&#038;s=0&#038;c=20201002" alt="L" title="L" class="latex" /> is some loss function. Let&#8217;s focus on ordinary regression, with squared loss. In this case, it is well-known that ordinary Bayesian linear regression with Gaussian priors scaled by the standard deviation of each feature is the same as using dropout. It&#8217;s the same with Gaussian process regression, since Gaussian process regression is identical to weight-space linear regression. But, let&#8217;s look at it in terms of output noise instead of weight noise. So, in this case, we are trying to minimize</p>
<p><img src="https://s0.wp.com/latex.php?latex=%5Cmathbb%7BE%7D_q%5B+%5Cfrac%7B1%7D%7B2%7D%7C%7C%5Cmathbf%7By%7D+-+%5Cmathbf%7B%5Chat%7By%7D%7D%7C%7C%5E2%5D+%3D+%5Cmathbb%7BE%7D_q%5B+%7C%7C%5Cmathbf%7By%7D+-+%281%2BA%29%5Cmathbf%7BK+%5Calpha%7D%7C%7C%5E2%5D&#038;bg=ffffff&#038;fg=111111&#038;s=0&#038;c=20201002" alt="&#92;mathbb{E}_q[ &#92;frac{1}{2}||&#92;mathbf{y} - &#92;mathbf{&#92;hat{y}}||^2] = &#92;mathbb{E}_q[ ||&#92;mathbf{y} - (1+A)&#92;mathbf{K &#92;alpha}||^2]" title="&#92;mathbb{E}_q[ &#92;frac{1}{2}||&#92;mathbf{y} - &#92;mathbf{&#92;hat{y}}||^2] = &#92;mathbb{E}_q[ ||&#92;mathbf{y} - (1+A)&#92;mathbf{K &#92;alpha}||^2]" class="latex" /></p>
<p>We can rewrite this as follows:</p>
<p><img src="https://s0.wp.com/latex.php?latex=%5Cmathbb%7BE%7D_q%5B+%5Cfrac%7B1%7D%7B2%7D%28%5Cmathbf%7By%7D+-+%5Cmathbf%7BK+%5Calpha%7D+%2B+%5Cmathbf%7BA%7D+%5Cmathbf%7BK+%5Calpha%7D%29%5E2%5D&#038;bg=ffffff&#038;fg=111111&#038;s=0&#038;c=20201002" alt="&#92;mathbb{E}_q[ &#92;frac{1}{2}(&#92;mathbf{y} - &#92;mathbf{K &#92;alpha} + &#92;mathbf{A} &#92;mathbf{K &#92;alpha})^2]" title="&#92;mathbb{E}_q[ &#92;frac{1}{2}(&#92;mathbf{y} - &#92;mathbf{K &#92;alpha} + &#92;mathbf{A} &#92;mathbf{K &#92;alpha})^2]" class="latex" />.</p>
<p>The last term involving the diagonal normal matrix <img src="https://s0.wp.com/latex.php?latex=%5Cmathbf%7BA%7D&#038;bg=ffffff&#038;fg=111111&#038;s=0&#038;c=20201002" alt="&#92;mathbf{A}" title="&#92;mathbf{A}" class="latex" /> is the only stochastic term. Now, if we expand this out, noting that <img src="https://s0.wp.com/latex.php?latex=A&#038;bg=ffffff&#038;fg=111111&#038;s=0&#038;c=20201002" alt="A" title="A" class="latex" /> has a mean of zero and its distribution is symmetric about the mean, we have,</p>
<p><img src="https://s0.wp.com/latex.php?latex=%5Cmathbb%7BE%7D_q%5B+%5Cfrac%7B1%7D%7B2%7D%7C%7C%5Cmathbf%7By%7D+-+%5Cmathbf%7BK+%5Calpha%7D+%2B+%5Cmathbf%7BA%7D+%5Cmathbf%7BK+%5Calpha%7D%7C%7C%5E2%5D+%3D+%7C%7C%5Cmathbf%7By%7D+-+%5Cmathbf%7BK+%5Calpha%7D%7C%7C%5E2+%2B+%5Cmathbb%7BE%7D_q%28%7C%7C%5Cmathbf%7BA%7D+%5Cmathbf%7BK+%5Calpha%7D%7C%7C%5E2%29&#038;bg=ffffff&#038;fg=111111&#038;s=0&#038;c=20201002" alt="&#92;mathbb{E}_q[ &#92;frac{1}{2}||&#92;mathbf{y} - &#92;mathbf{K &#92;alpha} + &#92;mathbf{A} &#92;mathbf{K &#92;alpha}||^2] = ||&#92;mathbf{y} - &#92;mathbf{K &#92;alpha}||^2 + &#92;mathbb{E}_q(||&#92;mathbf{A} &#92;mathbf{K &#92;alpha}||^2)" title="&#92;mathbb{E}_q[ &#92;frac{1}{2}||&#92;mathbf{y} - &#92;mathbf{K &#92;alpha} + &#92;mathbf{A} &#92;mathbf{K &#92;alpha}||^2] = ||&#92;mathbf{y} - &#92;mathbf{K &#92;alpha}||^2 + &#92;mathbb{E}_q(||&#92;mathbf{A} &#92;mathbf{K &#92;alpha}||^2)" class="latex" />.</p>
<p>Now, the expectation in the second term reduces to <img src="https://s0.wp.com/latex.php?latex=%5Csigma%5E2+%7C%7C%5Cmathbf%7BK%7D+%5Calpha%7C%7C&#038;bg=ffffff&#038;fg=111111&#038;s=0&#038;c=20201002" alt="&#92;sigma^2 ||&#92;mathbf{K} &#92;alpha||" title="&#92;sigma^2 ||&#92;mathbf{K} &#92;alpha||" class="latex" />. That means we are trying to minimize this:</p>
<p><img src="https://s0.wp.com/latex.php?latex=%5Cmathbb%7BE%7D_q%5B+%5Cfrac%7B1%7D%7B2%7D%7C%7C%5Cmathbf%7By%7D+-+%5Cmathbf%7BK+%5Calpha%7D+%2B+%5Cmathbf%7BA%7D+%5Cmathbf%7BK+%5Calpha%7D%7C%7C%5E2%5D+%3D+%5Cfrac%7B1%7D%7B2%7D%7C%7C%5Cmathbf%7By%7D+-+%5Cmathbf%7BK+%5Calpha%7D%7C%7C%5E2+%2B+%5Cfrac%7B1%7D%7B2%7D%5Csigma%5E2+%7C%7C%5Cmathbf%7BK%7D+%5Calpha%7C%7C%5E2&#038;bg=ffffff&#038;fg=111111&#038;s=0&#038;c=20201002" alt="&#92;mathbb{E}_q[ &#92;frac{1}{2}||&#92;mathbf{y} - &#92;mathbf{K &#92;alpha} + &#92;mathbf{A} &#92;mathbf{K &#92;alpha}||^2] = &#92;frac{1}{2}||&#92;mathbf{y} - &#92;mathbf{K &#92;alpha}||^2 + &#92;frac{1}{2}&#92;sigma^2 ||&#92;mathbf{K} &#92;alpha||^2" title="&#92;mathbb{E}_q[ &#92;frac{1}{2}||&#92;mathbf{y} - &#92;mathbf{K &#92;alpha} + &#92;mathbf{A} &#92;mathbf{K &#92;alpha}||^2] = &#92;frac{1}{2}||&#92;mathbf{y} - &#92;mathbf{K &#92;alpha}||^2 + &#92;frac{1}{2}&#92;sigma^2 ||&#92;mathbf{K} &#92;alpha||^2" class="latex" />.</p>
<p>This looks a lot like ordinary ridge regression/Gaussian process regression. Differentiating this, and setting the gradient to zero yields this system:</p>
<p><img src="https://s0.wp.com/latex.php?latex=%28%5Cmathbf%7BK%7D+%2B+%5Csigma%5E2+%5Cmathbf%7BK%7D%29%5Cmathbf%7B%5Calpha%7D+%3D+%5Cmathbf%7By%7D&#038;bg=ffffff&#038;fg=111111&#038;s=0&#038;c=20201002" alt="(&#92;mathbf{K} + &#92;sigma^2 &#92;mathbf{K})&#92;mathbf{&#92;alpha} = &#92;mathbf{y}" title="(&#92;mathbf{K} + &#92;sigma^2 &#92;mathbf{K})&#92;mathbf{&#92;alpha} = &#92;mathbf{y}" class="latex" /></p>
<p>The solution is,</p>
<p><img src="https://s0.wp.com/latex.php?latex=%5Calpha+%3D+%5Cfrac%7B1%7D%7B1%2B%5Csigma%5E2%7D%5Cmathbf%7BK%7D%5E%7B-1%7D%5Cmathbf%7By%7D&#038;bg=ffffff&#038;fg=111111&#038;s=0&#038;c=20201002" alt="&#92;alpha = &#92;frac{1}{1+&#92;sigma^2}&#92;mathbf{K}^{-1}&#92;mathbf{y}" title="&#92;alpha = &#92;frac{1}{1+&#92;sigma^2}&#92;mathbf{K}^{-1}&#92;mathbf{y}" class="latex" /></p>
<p>This just means that all of the weights are shrunk by a factor of <img src="https://s0.wp.com/latex.php?latex=1%2F%281%2B%5Csigma%5E2%29&#038;bg=ffffff&#038;fg=111111&#038;s=0&#038;c=20201002" alt="1/(1+&#92;sigma^2)" title="1/(1+&#92;sigma^2)" class="latex" /> compared to the unregularized solution. This is exactly the same thing as shrinkage in gradient boosting, which itself is usually thought of as a learning rate when viewing gradient boosting as gradient descent in function space.</p>
<p>Now, if we diagonalize just like before, the eigenvalues are also expanded by the same factor (corresponding to the same weight shrinkage as in the original reference frame). This means that dropout with output noising does absolutely nothing to improve conditioning of our problem. The condition number of the matrix, which is equal to the ratio of the largest to smallest eigenvalue, is unchanged. It does regularize the solution, in the more statistical sense of the word, in that it is robust to noise. But that is only a half of the reason why regularization is usually applied to kernel methods.</p>
<p>From before, here is the solution for a <img src="https://s0.wp.com/latex.php?latex=%5Calpha%5E%2A_i&#038;bg=ffffff&#038;fg=111111&#038;s=0&#038;c=20201002" alt="&#92;alpha^*_i" title="&#92;alpha^*_i" class="latex" /> in the diagonalized system with regular L2-regularized regression.</p>
<p><img src="https://s0.wp.com/latex.php?latex=%5Calpha%5E%2A_i+%3D+%5Cfrac%7By%5E%2A_i%7D%7B%5Clambda_i+%2B+%5Cxi_i%7D&#038;bg=ffffff&#038;fg=111111&#038;s=0&#038;c=20201002" alt="&#92;alpha^*_i = &#92;frac{y^*_i}{&#92;lambda_i + &#92;xi_i}" title="&#92;alpha^*_i = &#92;frac{y^*_i}{&#92;lambda_i + &#92;xi_i}" class="latex" /></p>
<p>Now, the corresponding solution for the case of multiplicative output noising is,</p>
<p><img src="https://s0.wp.com/latex.php?latex=%5Calpha%5E%2A_i+%3D+%5Cfrac%7By%5E%2A_i%7D%7B%5Clambda_i%7D%5Cfrac%7B1%7D%7B1%2B%5Csigma%5E2%7D+%3D+%5Cfrac%7By%5E%2A_i%7D%7B%5Clambda_i+%2B+%5Clambda_i+%5Csigma_i%5E2%7D.&#038;bg=ffffff&#038;fg=111111&#038;s=0&#038;c=20201002" alt="&#92;alpha^*_i = &#92;frac{y^*_i}{&#92;lambda_i}&#92;frac{1}{1+&#92;sigma^2} = &#92;frac{y^*_i}{&#92;lambda_i + &#92;lambda_i &#92;sigma_i^2}." title="&#92;alpha^*_i = &#92;frac{y^*_i}{&#92;lambda_i}&#92;frac{1}{1+&#92;sigma^2} = &#92;frac{y^*_i}{&#92;lambda_i + &#92;lambda_i &#92;sigma_i^2}." class="latex" /></p>
<p>So, in this reference frame, multiplicative output noising is the same as L2 regularization, but scaled by the corresponding eigenvalue. This is of course the same result as the original reference frame. However, it allows us to make an easy comparison between early stopping and multiplicative output noising. Suppose we are trying to fit <img src="https://s0.wp.com/latex.php?latex=%5Calpha_i%5E%2A&#038;bg=ffffff&#038;fg=111111&#038;s=0&#038;c=20201002" alt="&#92;alpha_i^*" title="&#92;alpha_i^*" class="latex" /> (in the diagonal reference frame) in unregularized Gaussian process regression (with squared-error loss) by gradient descent with a constant learning rate. If we take the learning rate to be very small and the iterations large, we can treat it as an ODE for gradient flow, which is easier. We&#8217;ll take the learning rate as <img src="https://s0.wp.com/latex.php?latex=%5Ceta&#038;bg=ffffff&#038;fg=111111&#038;s=0&#038;c=20201002" alt="&#92;eta" title="&#92;eta" class="latex" />. For squared error loss, we have the following equation,</p>
<p><img src="https://s0.wp.com/latex.php?latex=%5Cdot%7B%5Calpha%7D_i%5E%2A+%3D+%5Ceta%28y%5E%5Cstar_i+-+%5Clambda+%5Calpha_i%5E%2A%29&#038;bg=ffffff&#038;fg=111111&#038;s=0&#038;c=20201002" alt="&#92;dot{&#92;alpha}_i^* = &#92;eta(y^&#92;star_i - &#92;lambda &#92;alpha_i^*)" title="&#92;dot{&#92;alpha}_i^* = &#92;eta(y^&#92;star_i - &#92;lambda &#92;alpha_i^*)" class="latex" /></p>
<p>The solution to this with <img src="https://s0.wp.com/latex.php?latex=%5Calpha%5E%2A_i%280%29%3D0&#038;bg=ffffff&#038;fg=111111&#038;s=0&#038;c=20201002" alt="&#92;alpha^*_i(0)=0" title="&#92;alpha^*_i(0)=0" class="latex" /> is,</p>
<p><img src="https://s0.wp.com/latex.php?latex=%5Calpha%5E%2A_i%28t%29+%3D+%5Cfrac%7By%5E%2A_i%7D%7B%5Clambda_i%7D%281+-+%5Cexp%28-%5Ceta+%5Clambda_i+t%29%29&#038;bg=ffffff&#038;fg=111111&#038;s=0&#038;c=20201002" alt="&#92;alpha^*_i(t) = &#92;frac{y^*_i}{&#92;lambda_i}(1 - &#92;exp(-&#92;eta &#92;lambda_i t))" title="&#92;alpha^*_i(t) = &#92;frac{y^*_i}{&#92;lambda_i}(1 - &#92;exp(-&#92;eta &#92;lambda_i t))" class="latex" /> (10).</p>
<p>The solution as <img src="https://s0.wp.com/latex.php?latex=t&#038;bg=ffffff&#038;fg=111111&#038;s=0&#038;c=20201002" alt="t" title="t" class="latex" /> approaches infinity is the same as the unregularized problem. Also, the speed of convergence of depends exponentially on <img src="https://s0.wp.com/latex.php?latex=%5Clambda_i&#038;bg=ffffff&#038;fg=111111&#038;s=0&#038;c=20201002" alt="&#92;lambda_i" title="&#92;lambda_i" class="latex" />. This means that the solution for small eigenvalues converges exponentially more slowly than the big ones. When you also think that the spectrum of <img src="https://s0.wp.com/latex.php?latex=%5Cmathbf%7BK%7D&#038;bg=ffffff&#038;fg=111111&#038;s=0&#038;c=20201002" alt="&#92;mathbf{K}" title="&#92;mathbf{K}" class="latex" /> decays exponentially, it&#8217;s easy to see why plain-Jane gradient descent will never fit the components associated with the small eigenvalues in any reasonable number of iterations. This is the reason why I am using Adagrad for this model, which partially ameriolates this problem by using per-parameter adaptive learning rates. More on this in a bit.</p>
<p>From the previous equation, early stopping at time <img src="https://s0.wp.com/latex.php?latex=t&#038;bg=ffffff&#038;fg=111111&#038;s=0&#038;c=20201002" alt="t" title="t" class="latex" /> reduces <img src="https://s0.wp.com/latex.php?latex=%5Calpha%5E%2A_i%28t%29&#038;bg=ffffff&#038;fg=111111&#038;s=0&#038;c=20201002" alt="&#92;alpha^*_i(t)" title="&#92;alpha^*_i(t)" class="latex" /> by a factor of <img src="https://s0.wp.com/latex.php?latex=%281+-+%5Cexp%28%5Ceta+%5Clambda_i+t%29&#038;bg=ffffff&#038;fg=111111&#038;s=0&#038;c=20201002" alt="(1 - &#92;exp(&#92;eta &#92;lambda_i t)" title="(1 - &#92;exp(&#92;eta &#92;lambda_i t)" class="latex" />! This is exactly the same thing as using multiplicative output noise with <img src="https://s0.wp.com/latex.php?latex=%5Csigma&#038;bg=ffffff&#038;fg=111111&#038;s=0&#038;c=20201002" alt="&#92;sigma" title="&#92;sigma" class="latex" /> chosen such that,</p>
<p><img src="https://s0.wp.com/latex.php?latex=%281+-+%5Cexp%28%5Ceta+%5Clambda_i+t%29%29+%3D+%5Cfrac%7B1%7D%7B1+%2B+%5Csigma%5E2%7D&#038;bg=ffffff&#038;fg=111111&#038;s=0&#038;c=20201002" alt="(1 - &#92;exp(&#92;eta &#92;lambda_i t)) = &#92;frac{1}{1 + &#92;sigma^2}" title="(1 - &#92;exp(&#92;eta &#92;lambda_i t)) = &#92;frac{1}{1 + &#92;sigma^2}" class="latex" /></p>
<p>Of course, as I said before, it&#8217;s impossible to choose the proper learning rate <img src="https://s0.wp.com/latex.php?latex=%5Ceta&#038;bg=ffffff&#038;fg=111111&#038;s=0&#038;c=20201002" alt="&#92;eta" title="&#92;eta" class="latex" /> without knowing the exact solution already. However, if we know the eigenvalues, we can think about what a good learning rate might look like. Suppose that for an eigenvalue of unity, with a learning rate of unity, we converge to our early-stopping point,</p>
<p><img src="https://s0.wp.com/latex.php?latex=%5Cfrac%7By%7D%7B1%7D%5Cfrac%7B1%7D%7B1+%2B+%5Csigma%5E2%7D&#038;bg=ffffff&#038;fg=111111&#038;s=0&#038;c=20201002" alt="&#92;frac{y}{1}&#92;frac{1}{1 + &#92;sigma^2}" title="&#92;frac{y}{1}&#92;frac{1}{1 + &#92;sigma^2}" class="latex" />,</p>
<p>after a time of unity. Then suppose that instead of an eigenvalue of unity, we have <img src="https://s0.wp.com/latex.php?latex=%5Clambda_i&#038;bg=ffffff&#038;fg=111111&#038;s=0&#038;c=20201002" alt="&#92;lambda_i" title="&#92;lambda_i" class="latex" /> as before, which can be arbitrary. We also want to have,</p>
<p><img src="https://s0.wp.com/latex.php?latex=1+-+%5Cexp%28%5Ceta_i+%5Clambda_i+t%29+%3D+%5Cfrac%7B1%7D%7B1+%2B+%5Csigma%5E2%7D&#038;bg=ffffff&#038;fg=111111&#038;s=0&#038;c=20201002" alt="1 - &#92;exp(&#92;eta_i &#92;lambda_i t) = &#92;frac{1}{1 + &#92;sigma^2}" title="1 - &#92;exp(&#92;eta_i &#92;lambda_i t) = &#92;frac{1}{1 + &#92;sigma^2}" class="latex" />,</p>
<p>after a time of <img src="https://s0.wp.com/latex.php?latex=t%3D1&#038;bg=ffffff&#038;fg=111111&#038;s=0&#038;c=20201002" alt="t=1" title="t=1" class="latex" />. Then we can simply solve for the learning rate <img src="https://s0.wp.com/latex.php?latex=%5Ceta_i&#038;bg=ffffff&#038;fg=111111&#038;s=0&#038;c=20201002" alt="&#92;eta_i" title="&#92;eta_i" class="latex" />, yielding,</p>
<p><img src="https://s0.wp.com/latex.php?latex=%5Ceta_i+%3D+%5Cfrac%7B%5Clog%281+-+%5Cfrac%7B1%7D%7B1%2B%5Csigma%5E2%7D%29%7D%7B%5Clambda_i%7D&#038;bg=ffffff&#038;fg=111111&#038;s=0&#038;c=20201002" alt="&#92;eta_i = &#92;frac{&#92;log(1 - &#92;frac{1}{1+&#92;sigma^2})}{&#92;lambda_i}" title="&#92;eta_i = &#92;frac{&#92;log(1 - &#92;frac{1}{1+&#92;sigma^2})}{&#92;lambda_i}" class="latex" /></p>
<p>Thus, the learning rate should be inversely proportional to the eigenvalue. This is also what Newton&#8217;s method does. Of course, in real life, we would not have <img src="https://s0.wp.com/latex.php?latex=%5Cmathbf%7BK%7D&#038;bg=ffffff&#038;fg=111111&#038;s=0&#038;c=20201002" alt="&#92;mathbf{K}" title="&#92;mathbf{K}" class="latex" /> diagonalized. However, <a href="https://arxiv.org/abs/1307.1493">Wager et al. 2013</a> showed dropout for GLMs is first-order equivalent to gradient descent with the learning rate scaled by the inverse diagonal of the Hessian, which is an approximation to Newton&#8217;s method (or the natural gradient). In gradient descent element <img src="https://s0.wp.com/latex.php?latex=h_i&#038;bg=ffffff&#038;fg=111111&#038;s=0&#038;c=20201002" alt="h_i" title="h_i" class="latex" /> of the Hessian&#8217;s diagonal can be estimated by,</p>
<p><img src="https://s0.wp.com/latex.php?latex=h_i+%5Capprox+%5Cfrac%7B1%7D%7B%5Csum_i%5En+g%5Ej_i+g%5Ej_i%7D&#038;bg=ffffff&#038;fg=111111&#038;s=0&#038;c=20201002" alt="h_i &#92;approx &#92;frac{1}{&#92;sum_i^n g^j_i g^j_i}" title="h_i &#92;approx &#92;frac{1}{&#92;sum_i^n g^j_i g^j_i}" class="latex" />,</p>
<p>where <img src="https://s0.wp.com/latex.php?latex=g_i%5Ej&#038;bg=ffffff&#038;fg=111111&#038;s=0&#038;c=20201002" alt="g_i^j" title="g_i^j" class="latex" /> is the gradient of of weight <img src="https://s0.wp.com/latex.php?latex=i&#038;bg=ffffff&#038;fg=111111&#038;s=0&#038;c=20201002" alt="i" title="i" class="latex" />, at iteration <img src="https://s0.wp.com/latex.php?latex=j&#038;bg=ffffff&#038;fg=111111&#038;s=0&#038;c=20201002" alt="j" title="j" class="latex" />. Adagrad is not so far off from this. With Adagrad, the learning rate is inversely proportional to the square root of sum of gradients at each step before that point. This means that components with small gradients (here, small eigenvalues) have their learning rates reduced less than components with big gradients (here, big eigenvalues).</p>
<p>This explains why early stopping with Adagrad seemed to be working surprisingly well for me.</p>
<p>In contrast, early stopping is completely outmoded for DNNs, and it makes sense why: With nice convex problems where you are basically walking to the bottom of a bowl, early stopping with Adagrad appears to be a decent regularizer: If you start from weights near zero, you walk down the bowl, with the length of your stride in each direction scaled by the curvature. If your bowl is more like a canoe, using Adagrad is basically like squashing the long axis to be closer to the short axis.</p>
<p>This analogy doesn&#8217;t work for DNNs. They have a ton of minima, and a complicated landscape. It&#8217;s not like walking down a bowl, but more like walking down a twisty canyon. The distance you have travelled doesn&#8217;t bear much relation to how close you are to the optimum. Hence why early stopping doesn&#8217;t work well.</p>
</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered">
<div class="prompt input_prompt"></div>
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<h1 id="Conclusions">Conclusions</h1>
</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered">
<div class="prompt input_prompt"></div>
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<ul>
<li>Dropout is a bit disappointing for kernel models. The stochasticity introduced slows down training a lot, on top of having to use gradient descent instead of faster second-order methods. It also doesn&#8217;t help ill-conditioning of the kernel matrix.</li>
<li>However, dropout is an easy way to get Bayesian gaussian process GLMs using stochastic variational inference (not that stochastic additive noise couldn&#8217;t be used as well). It ought to yield significantly more accurate posterior distributions than the Laplace approximation used by Scikit-learn. However, other methods like expectation propagation should be superior in every way.</li>
<li>For squared-error loss, multiplicative output noise is the same as just multiplying the original kernel matrix by factor, or multiplicatively shrinking the weights by a predetermined amount.</li>
<li>For generalized Gaussian process regression, it is hard to overfit with gradient descent, since it takes forever to fit the parts of the objective corresponding to small eigenvalues (when linearizing at each iteration). Neural networks tend to have much fewer parameters than the data size, unlike Gaussian processes, which helps ameriolate this problem.</li>
<li>Early stopping with Adagrad with no regularization works surprisingly well. It&#8217;s not as good as standard methods of regularization optimized with second-order methods, but its pretty interesting nonetheless.</li>
</ul>
</div>
</div>
